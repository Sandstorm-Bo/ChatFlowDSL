"""
æ··åˆåŒ¹é…æœºåˆ¶æ¼”ç¤ºè„šæœ¬

å±•ç¤º"è§„åˆ™ä¼˜å…ˆ + LLMå…œåº•"çš„å¼ºå¤§èƒ½åŠ›
"""

import sys
import yaml
from core.chatbot import Chatbot
from llm.llm_responder import LLMResponder


def print_banner(text):
    """æ‰“å°ç¾åŒ–çš„æ ‡é¢˜"""
    print(f"\n{'='*80}")
    print(f"  {text}")
    print(f"{'='*80}\n")


def demo_rule_matching():
    """æ¼”ç¤º1: è§„åˆ™åŒ¹é…ï¼ˆå¿«é€Ÿã€ç²¾ç¡®ï¼‰"""
    print_banner("æ¼”ç¤º1: è§„åˆ™åŒ¹é… - æ ‡å‡†è¡¨è¾¾")

    chatbot = Chatbot(flows_dir="dsl/flows", llm_responder=None)

    test_cases = [
        "ä½ å¥½",
        "æˆ‘æƒ³äº†è§£äº§å“",
        "æŸ¥è¯¢è®¢å•A1234567890",
        "æˆ‘è¦é€€æ¬¾",
    ]

    for user_input in test_cases:
        print(f"ğŸ‘¤ ç”¨æˆ·: {user_input}")
        responses = chatbot.handle_message(f"demo-rule-{user_input[:5]}", user_input)
        print(f"ğŸ¤– ç³»ç»Ÿ: {responses[0] if responses else '(æ— å›å¤)'}\n")


def demo_llm_fallback():
    """æ¼”ç¤º2: LLMå…œåº•ï¼ˆç†è§£å£è¯­åŒ–è¡¨è¾¾ï¼‰"""
    print_banner("æ¼”ç¤º2: LLMå…œåº• - å£è¯­åŒ–è¡¨è¾¾")

    # åŠ è½½é…ç½®
    try:
        with open("config/config.yaml", 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print("âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: config/config.yaml")
        print("æç¤º: è¯·å…ˆé…ç½®LLM API Key")
        return

    llm_config = config.get("llm", {})
    if not llm_config.get("api_key"):
        print("âš ï¸ æœªé…ç½®LLM API Key")
        print("æç¤º: è¯·åœ¨config/config.yamlä¸­é…ç½®llm.api_key")
        return

    # åˆå§‹åŒ–LLMå“åº”å™¨
    llm_responder = LLMResponder(
        api_key=llm_config["api_key"],
        model_name=llm_config["model_name"],
        base_url=llm_config.get("base_url"),
        timeout=llm_config.get("timeout", 30)
    )

    chatbot = Chatbot(flows_dir="dsl/flows", llm_responder=llm_responder)

    # å£è¯­åŒ–æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "input": "é‚£ä¸ªå•å­å‘åˆ°å“ªäº†",
            "note": "å£è¯­åŒ–è¡¨è¾¾ï¼š'å•å­' = è®¢å•"
        },
        {
            "input": "ä¸œè¥¿åäº†æƒ³é€€",
            "note": "ç®€åŒ–è¡¨è¾¾ï¼šæƒ³é€€æ¬¾"
        },
        {
            "input": "å¸®æˆ‘çœ‹çœ‹ä½ ä»¬å–å•¥",
            "note": "å£è¯­åŒ–ï¼š'å–å•¥' = æœ‰ä»€ä¹ˆäº§å“"
        },
    ]

    for case in test_cases:
        print(f"ğŸ‘¤ ç”¨æˆ·: {case['input']}")
        print(f"   ğŸ’¡ {case['note']}")
        responses = chatbot.handle_message(f"demo-llm-{case['input'][:5]}", case["input"])
        print(f"ğŸ¤– ç³»ç»Ÿ: {responses[0] if responses else '(æ— å›å¤)'}\n")


def demo_performance_comparison():
    """æ¼”ç¤º3: æ€§èƒ½å¯¹æ¯”"""
    print_banner("æ¼”ç¤º3: æ€§èƒ½å¯¹æ¯” - è§„åˆ™ vs LLM")

    import time

    # çº¯è§„åˆ™åŒ¹é…
    chatbot_rule = Chatbot(flows_dir="dsl/flows", llm_responder=None)

    test_input = "æŸ¥è¯¢è®¢å•A1234567890"
    print(f"æµ‹è¯•è¾“å…¥: {test_input}")

    start = time.time()
    chatbot_rule.handle_message("perf-test-rule", test_input)
    rule_time = (time.time() - start) * 1000

    print(f"\nâœ“ è§„åˆ™åŒ¹é…: {rule_time:.2f}ms")
    print(f"  - ä¼˜åŠ¿: æå¿«å“åº”")
    print(f"  - é€‚ç”¨: æ ‡å‡†è¡¨è¾¾")

    # å°è¯•LLMæ¨¡å¼ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    try:
        with open("config/config.yaml", 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        llm_config = config.get("llm", {})
        if llm_config.get("api_key"):
            llm_responder = LLMResponder(
                api_key=llm_config["api_key"],
                model_name=llm_config["model_name"],
                base_url=llm_config.get("base_url"),
                timeout=llm_config.get("timeout", 30)
            )

            chatbot_hybrid = Chatbot(flows_dir="dsl/flows", llm_responder=llm_responder)

            llm_test_input = "é‚£ä¸ªå•å­åˆ°å“ªäº†"
            print(f"\næµ‹è¯•è¾“å…¥: {llm_test_input} (å£è¯­åŒ–)")

            start = time.time()
            chatbot_hybrid.handle_message("perf-test-llm", llm_test_input)
            llm_time = (time.time() - start) * 1000

            print(f"\nâœ“ LLMå…œåº•: {llm_time:.2f}ms")
            print(f"  - ä¼˜åŠ¿: ç†è§£è¯­ä¹‰")
            print(f"  - é€‚ç”¨: å£è¯­åŒ–è¡¨è¾¾")

            print(f"\nğŸ“Š æ€§èƒ½å·®å¼‚: LLMè€—æ—¶çº¦ä¸ºè§„åˆ™çš„ {(llm_time/rule_time):.1f}x")
        else:
            print("\nâš ï¸ æœªé…ç½®LLMï¼Œè·³è¿‡LLMæ€§èƒ½æµ‹è¯•")

    except Exception as e:
        print(f"\nâš ï¸ LLMæµ‹è¯•å¤±è´¥: {str(e)}")


def interactive_demo():
    """æ¼”ç¤º4: äº¤äº’å¼ä½“éªŒ"""
    print_banner("æ¼”ç¤º4: äº¤äº’å¼ä½“éªŒ - è‡ªç”±å¯¹è¯")

    try:
        with open("config/config.yaml", 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except:
        config = {}

    llm_responder = None
    llm_config = config.get("llm", {})

    if llm_config.get("api_key"):
        llm_responder = LLMResponder(
            api_key=llm_config["api_key"],
            model_name=llm_config["model_name"],
            base_url=llm_config.get("base_url"),
            timeout=llm_config.get("timeout", 30)
        )
        print("âœ“ LLMå“åº”å™¨å·²å¯ç”¨ï¼ˆæ··åˆæ¨¡å¼ï¼‰")
    else:
        print("â„¹ï¸ ä»…ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼ˆæ— LLMï¼‰")

    chatbot = Chatbot(flows_dir="dsl/flows", llm_responder=llm_responder)
    session_id = "interactive-demo"

    print("\næ‚¨å¯ä»¥è¾“å…¥ä»»ä½•é—®é¢˜ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³åŒ¹é…æ–¹å¼")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n")

    while True:
        try:
            user_input = input("ğŸ‘¤ æ‚¨: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['quit', 'exit', 'bye', 'é€€å‡º']:
                print("\nğŸ‘‹ å†è§ï¼")
                break

            responses = chatbot.handle_message(session_id, user_input)

            print(f"ğŸ¤– ç³»ç»Ÿ:")
            for response in responses:
                print(f"  {response}")
            print()

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}\n")


def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("â•”" + "â•"*78 + "â•—")
    print("â•‘" + " "*22 + "ChatFlowDSL æ··åˆåŒ¹é…æ¼”ç¤º" + " "*23 + "â•‘")
    print("â•‘" + " "*78 + "â•‘")
    print("â•‘" + " "*18 + "è§„åˆ™ä¼˜å…ˆ + LLMè¯­ä¹‰ç†è§£å…œåº•" + " "*20 + "â•‘")
    print("â•š" + "â•"*78 + "â•")

    print("\nè¯·é€‰æ‹©æ¼”ç¤ºæ¨¡å¼ï¼š")
    print("  1. è§„åˆ™åŒ¹é…æ¼”ç¤ºï¼ˆå¿«é€Ÿã€ç²¾ç¡®ï¼‰")
    print("  2. LLMå…œåº•æ¼”ç¤ºï¼ˆå£è¯­åŒ–ç†è§£ï¼‰")
    print("  3. æ€§èƒ½å¯¹æ¯”ï¼ˆè§„åˆ™ vs LLMï¼‰")
    print("  4. äº¤äº’å¼ä½“éªŒï¼ˆè‡ªç”±å¯¹è¯ï¼‰")
    print("  0. è¿è¡Œæ‰€æœ‰æ¼”ç¤º")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-4): ").strip()

    if choice == "1":
        demo_rule_matching()
    elif choice == "2":
        demo_llm_fallback()
    elif choice == "3":
        demo_performance_comparison()
    elif choice == "4":
        interactive_demo()
    elif choice == "0":
        demo_rule_matching()
        demo_llm_fallback()
        demo_performance_comparison()
        print("\næ˜¯å¦è¿›å…¥äº¤äº’å¼ä½“éªŒï¼Ÿ(y/n): ", end="")
        if input().lower() == 'y':
            interactive_demo()
    else:
        print("æ— æ•ˆé€‰é¡¹")

    print("\n" + "="*80)
    print("  æ¼”ç¤ºç»“æŸï¼")
    print("="*80)
    print("\nğŸ“š æ›´å¤šæ–‡æ¡£:")
    print("  - æ··åˆåŒ¹é…æŒ‡å—: docs/HYBRID_MATCHING_GUIDE.md")
    print("  - DSLè¯­æ³•è§„èŒƒ: docs/DSL_SPECIFICATION.md")
    print("  - é¡¹ç›®æ–‡æ¡£: docs/PROJECT_DOCUMENTATION.md")
    print()


if __name__ == "__main__":
    main()
